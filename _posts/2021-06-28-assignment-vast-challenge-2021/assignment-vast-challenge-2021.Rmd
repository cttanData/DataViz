---
title: "Assignment - VAST Challenge 2021"
description: |
  [VAST Challenge 2021](https://vast-challenge.github.io/2021/). Applying visual analytics to gain Insight.....  
author:
  - name: Choo T Tan
    url: {}
date: 06-28-2021
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, error = FALSE, fig.retina = 3)
```

```{r echo = FALSE, warning = FALSE}
packages <- c('sf','tidyverse', 'igraph', 'plotly', 'ggiraph','lubridate', 'clock')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p,character.only = T)
}
```
```{r echo = FALSE, warning = FALSE    }
#Loading the base file 
credit <- read_csv("data/aspatial/cc_data.csv")
loyalty <- read_csv("data/aspatial/loyalty_data.csv")
#veh <- read_csv("data/car-assignments.csv")
gps <- read_csv("data/aspatial/gps.csv")
```

# 1.0 Introduction 
The assignment is based on [VAST Challenge 2021](https://vast-challenge.github.io/2021/) where it attempted to solve the [Mini Challenges 2 (MC2)](https://vast-challenge.github.io/2021/MC2.html) via visual analytics out of the 3 mini challenges. The fictitious scenario was based on a Tethys-based natural gas production company GAStech in the country of Kronos where it has remarkable profits and strong relationship with the government. However, GAStech has not been as successful in demonstrating environmental stewardship. Several Employee went missing in January 2014 after a successful initial public offering. It was suspected that the disappearance might be associated with an organization known as the Protectors of Kronos (POK). However, things may not be so simplicity. Thus,one is tasked to assist the law enforcement from Kronos and Tethys in solving the assigned task via visual analytics.

# 2.0 Liteture Review

# 3.0 Approach and Overview
For MC2, the main task was to identify suspicious patterns of behavior from the transaction done by GASTech employees. You must cope with uncertainties that result from missing, conflicting, and imperfect data to make recommendations for further investigation.there are 4 main CSV files with Korons geospatial data and it based on last 14 days, 6 - 19 January 2014 prior to the disappearance. The CSV files are namely (1) credit card transaction, (2) loyalty card transaction, (3) geospatial tracking data for company cars which not know to employee, (4) car assignment. The investigation will the following 4 main steps:

1. **Preliminary Analysis**. The intent is to gain insights across the dataset distribution and disparity. A basic understand on the foundation layer.

2. **Data Wrangling**. To establish relationship between credit card, loyalty card and geospatial tracking data, To prepare the data for subsequent analysis.

3. **Exploratory Data Analysis (EDA)**.  To identify patterns and anomalies from the transactions and geospatial tracking. Subsequent, to correlate the discovered insights in suitable narrative for further investigation. 

# 4.0 Preliminary Analysis

Sample of credit and loyalty datasets were shown below. Preliminary, observed that there were more credit than loyalty card transaction. Whether it was a scenario on independent credit card transaction without loyalty card or vice verse was something to be discovered later.Moreover credit card has transaction data time information while loyalty card only has transaction date only. Reference to the respective column datatype, need to covert "timestamp" to datetime/date,"last4ccnum" and loyaltynum" to factor as technically, it was not a numeric data type. When one explored the "timestamp" data deeper, its observed that the variable has different date format after 13 Jan. i.e. 2014 %Y vs 14 %y. Would need to be clean. 

Glimpse of credit card dataset:

```{r}
glimpse(credit)
```

Glimpse of loyalty card dataset:

```{r}
glimpse(loyalty)
```

```{r, echo=FALSE, results = FALSE}
length(unique(credit$last4ccnum))
length(unique(loyalty$loyaltynum))
length(unique(credit$location))
length(unique(loyalty$location))
credit$location[!(credit$location %in% loyalty$location)]
credit$price[!(credit$location %in% loyalty$location)]
```

Reference to the company employee records in challenge 1, GASTech has 54 staffs. However when do a unique count on the credit and loyalty card numbers, it was 55 and 54 count respectively. It alluded that one of the staff could have 2 credit cards. Moreover,a unique count on the number of transaction locations for each dataset has shown that there were 34 and 33 locations for credit and loyalty card respectively. The additional location in credit card dataset was known as "Daily Dealz" with a transaction amount "2.01".From the transaction price captured respectively in both dataset, interesting to note that the mean was relatively close but min-max and inter-quartile range were quite different. 

```{r}
print("Credit Card PriceTransaction Summary.")
summary(credit$price)
print("Loyalty Card Price Transaction Summary.")
summary(loyalty$price)
```

On the geospatial tracking dateset, similar observation on timestamp data type. Given there were 35 and 5 unique private company car and truck respectively, there would be some transaction credit/loyalty card that would have difficulty to associate it to an individual within the company since not everybody drives. Similar treatment on datatype would have to be done on "Timestamp" and "id".   

Glimpse of geospatial tracking dataset:

```{r}
glimpse(gps)
```

# 5.0 Data Wrangling
## 5.1 Preparation of Personnel Node
This will be built based on MC1 employee records. Only relevant information will be used. 

Glimpse of personnel information:

```{r, warning = FALSE}
#Loading the base file 
GAStech_persnode<- read_csv("data/aspatial/employeeinfo.csv")
glimpse(GAStech_persnode)
```
## 5.1 Mapping Credit to Loyalty Card

Perform required data wrangling based on observation in Section 4. To resolve the datetime format for timestamp and set the credit card number as factor. In addition, create date, weekday and hour for the credit. For loyalty card, renamed timestamp as date to faciltate inner join later. Similarly change the datatype for "loyaltynum".

Glimpse of prepared credit card dataset:

```{r clean up credit up, echo = FALSE}
credit$last4ccnum = as_factor(credit$last4ccnum) # convert it factor

#cleaning up on datetime format, since there is %y vs %Y. Try to use ifelse but syntac error that cant resolve
credit <- credit %>% # separate them into fine entity
  separate(timestamp, into = c("date","time"), sep=" ",remove = TRUE) %>%
  separate(time, into = c("hr", "min"), sep=":",remove = TRUE) %>%
  separate(date, into = c("mth", "day", "yr"), sep="/",remove = FALSE) %>%
  mutate(yr=2014)

pack <- c('yr','mth','day','hr','min') # convert to numeric since make_datetime need numeric format
for (p in pack){
  credit <- credit %>% mutate_at(c(p),as.numeric)
}

credit  <- credit %>% 
  mutate(timestamp = make_datetime(yr,mth,day,hr,min)) %>%
  dplyr::select(-yr,-mth,-min) %>%
  arrange(last4ccnum, timestamp) %>%
  relocate(last4ccnum, timestamp, .before=date)

credit$wkday <- lubridate::wday(credit$timestamp, label= TRUE, abbr = FALSE)

glimpse(credit)
```
```{r echo = FALSE, eval = FALSE}
#cant get it work
credit$last4ccnum <-  as_factor(credit$last4ccnum) # convert it factor
credit <- credit %>% # separate them into fine entity
  separate(timestamp, into = c("date","time"), sep=" ",remove = FALSE)%>%
  arrange(last4ccnum,timestamp)
credit$timestamp <- date_time_parse(credit$timestamp,  zone ="", format = "%m/%d/%Y %H:%M") # cover to dttm
year(credit$timestamp)=2014 # due to %y vs %Y
credit$date = dmy(credit$date)
credit$weekday = lubridate::wday(credit$date, label=TRUE, abbr=FALSE)
glimpse(credit)
```

Glimpse of prepared loyalty card dataset:

```{r clean up loyalty card, echo = FALSE}
loyalty$loyaltynum = as_factor(loyalty$loyaltynum)
loyalty<-loyalty %>% 
  rename(date = timestamp) %>%
  arrange(loyaltynum) %>%
  relocate(loyaltynum, .before = date)
glimpse(loyalty)
```

__Inner join for both Datasets__. Based on date, location and price for inner join. Thereafter, compute the number of distinct count between a credit-loyalty pair. If respective credit and loyalty cards has more than one distinct pair, imply the credit card owner could be using numerous loyalty cards or vice verse. It note that inner join has 1087 observations, 403 and 305 less than credit and loyalty card transaction respectively. This could imply that individual could only transact with one card due to some reasons. When determine the unique count on both cards and location, it was determined that the unique count for credit and loyalty card remained no change. This imply all possible relationship between both card should be captured within the inner join. On location, the inner join has 2 location less than the credit card transaction, namely, Korons Mart and Daily Dealz. This implied that at any onetime, only one card has been used in those places.

```{r, inner join Credit and Loyalty to determine relationship, echo=TRUE}
ijoin <- credit %>%
  inner_join(loyalty,by=c('date', 'location','price')) %>%
  group_by(last4ccnum) %>%
  mutate(ndist = n_distinct(loyaltynum)) %>%
  ungroup() %>%
  group_by(loyaltynum) %>%
  mutate(ndist = if_else(ndist==1,n_distinct(last4ccnum), ndist)) %>%
  ungroup()

ijoin

```
```{r, results=FALSE}
length(unique(ijoin$last4ccnum))
length(unique(ijoin$loyaltynum))
length(unique(ijoin$location))
credit$location[!(credit$location %in% ijoin$location)]
```
```{r message = FALSE, warning = FALSE}
#forming the bipartite graph
bigraph <- ijoin %>% # filter out unique pairing and count
  filter(ndist >= 2)%>%
  group_by(last4ccnum,loyaltynum) %>%
  summarise(weights=n()) %>%
  mutate_at(c("weights"), as.numeric)%>%
  arrange(loyaltynum) %>%
  ungroup()
#plot bipartite graph
bg<- graph.data.frame(bigraph, directed = FALSE)
V(bg)$type <- bipartite_mapping(bg)$type # assign vertex or nodes
shape <- c("circle", "square")
E(bg)$color[E(bg)$weights<=5] <- 'red'
E(bg)$color[E(bg)$weights>5] <- 'sky blue'
```

__Bipartite Graph for Credit-Loyalty Pair__. group_by was done on the inner join dataset to determine the number of transactions for each unique pairing of credit and loyalty card. To minimise cluttering in discovering the unique pairing respective credit to loyalty card, we have filtered the dataset for distinct pairing count >=2. Will use igraph to built a bipartite graph with nodes as credit and loyalty cards, where the edges connote transaction where both cards were used and the edge weight equal to the number of transactions. The bipartite graph was shown below.

```{r message = FALSE,layout="l-body",fig.height = 7, fig.align ="centre", fig.cap = "Bipartite Graph of Credit(Orange) and Loyalty(Pink) Cards.", warning = FALSE}
plot(bg, layout=layout.bipartite, 
     vertex.color=c("orange","pink")[V(bg)$type+1], vertex.size=20, vertex.label.cex=0.9,
     vertex.shape = shape[V(bg)$type+1],
     edge.width = E(bg)$weights*0.8,
     edge.label=E(bg)$weights, edge.label.cex=0.9, edge.label.color ="black")
#layout="l-screen-inset"
```


